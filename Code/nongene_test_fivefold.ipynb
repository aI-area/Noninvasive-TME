{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test all model on Radiogenomics Validation Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc, r2_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import precision_recall_curve, precision_score, average_precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_results(y_true, y_pred):\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_score=y_pred, y_true=y_true)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    acc_con1 = confusion_matrix(y_pred=y_pred, y_true=y_true)\n",
    "    acc_pred1 = accuracy_score(y_pred=y_pred, y_true=y_true)\n",
    "\n",
    "    tn, fp, fn, tp = acc_con1.ravel()\n",
    "    # Specificity\n",
    "    spec_score = tn / (tn + fp) if (tn + fp) > 0 else 0 \n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_pred, y_true)\n",
    "    pr_auc = auc(recall, precision)\n",
    "\n",
    "    preci_scores = precision_score(y_pred=y_pred, y_true=y_true)\n",
    "    recall_scores = recall_score(y_pred=y_pred, y_true=y_true)\n",
    "\n",
    "    #print(\"Accuracy Test: \", acc_pred1)\n",
    "    #print(\"Confusion Matrix: \")\n",
    "    #print(acc_con1) \n",
    "    #print(\"ROC AUC: \", roc_auc)\n",
    "    #print(\"PR AUC: \", pr_auc)\n",
    "\n",
    "    #print('Recall / Sensitivity: ', recall_scores)\n",
    "    #print('Specificity: ', spec_score)\n",
    "    return acc_pred1, roc_auc, pr_auc, recall_scores, spec_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "caf_results = pd.read_csv('../Results/nongene/caf_output.csv')\n",
    "tnbc_results = pd.read_csv('../Results/nongene/tnbc_output.csv')\n",
    "macro_results = pd.read_csv('../Results/nongene/macro_output.csv')\n",
    "cd4_results = pd.read_csv('../Results/nongene/cd4_output.csv')\n",
    "endo_results = pd.read_csv('../Results/nongene/endo_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Test:  0.7081081081081081 five fold:  [0.11711711711711711, 0.9009009009009009, 0.8468468468468469, 0.8288288288288288, 0.8468468468468469]\n",
      "ROC AUC:  0.5 five fold:  [0.5, 0.5, 0.5, 0.5, 0.5]\n",
      "PR AUC:  0.6 five fold:  [1.0, 0.5, 0.5, 0.5, 0.5]\n",
      "Recall / Sensitivity:  0.2 five fold:  [1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Specificity:  0.8 five fold:  [0.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:993: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:993: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:993: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:993: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# TNBC TEST\n",
    "acc_all = []\n",
    "pr_auc_all = []\n",
    "roc_acu_all = []\n",
    "recall_all = []\n",
    "spec_all = []\n",
    "\n",
    "for i in range(5):\n",
    "    pred_i = 'tnbc_pred_%s'%(i+1)\n",
    "    true_i = 'tnbc_true_%s'%(i+1)\n",
    "    tnbc_pred_i = np.array(tnbc_results[pred_i])\n",
    "    tnbc_true_i = np.array(tnbc_results[true_i])\n",
    "    \n",
    "    acc_pred1, roc_auc, pr_auc, recall_scores, spec_score = cal_results(tnbc_true_i, tnbc_pred_i)\n",
    "\n",
    "    acc_all.append(acc_pred1)\n",
    "    pr_auc_all.append(pr_auc)\n",
    "    roc_acu_all.append(roc_auc)\n",
    "    recall_all.append(recall_scores)\n",
    "    spec_all.append(spec_score)\n",
    "\n",
    "print(\"Accuracy Test: \", np.mean(acc_all),'five fold: ', acc_all)\n",
    "print(\"ROC AUC: \", np.mean(roc_acu_all), 'five fold: ', roc_acu_all)\n",
    "print(\"PR AUC: \", np.mean(pr_auc_all), 'five fold: ', pr_auc_all )\n",
    "print('Recall / Sensitivity: ', np.mean(recall_all), 'five fold: ', recall_all)\n",
    "print('Specificity: ', np.mean(spec_all), 'five fold: ', spec_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Test:  0.5045045045045045 five fold:  [0.5135135135135135, 0.5405405405405406, 0.4864864864864865, 0.4774774774774775, 0.5045045045045045]\n",
      "ROC AUC:  0.5 five fold:  [0.5, 0.5, 0.5, 0.5, 0.5]\n",
      "PR AUC:  0.6 five fold:  [1.0, 0.5, 0.5, 0.5, 0.5]\n",
      "Recall / Sensitivity:  0.2 five fold:  [1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Specificity:  0.8 five fold:  [0.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:993: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:993: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:993: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:993: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# CAF TEST\n",
    "acc_all = []\n",
    "pr_auc_all = []\n",
    "roc_acu_all = []\n",
    "recall_all = []\n",
    "spec_all = []\n",
    "\n",
    "for i in range(5):\n",
    "    pred_i = 'caf_pred_%s'%(i+1)\n",
    "    true_i = 'caf_true_%s'%(i+1)\n",
    "    caf_pred_i = np.array(caf_results[pred_i])\n",
    "    caf_true_i = np.array(caf_results[true_i])\n",
    "    \n",
    "    acc_pred1, roc_auc, pr_auc, recall_scores, spec_score = cal_results(caf_true_i, caf_pred_i)\n",
    "\n",
    "    acc_all.append(acc_pred1)\n",
    "    pr_auc_all.append(pr_auc)\n",
    "    roc_acu_all.append(roc_auc)\n",
    "    recall_all.append(recall_scores)\n",
    "    spec_all.append(spec_score)\n",
    "\n",
    "print(\"Accuracy Test: \", np.mean(acc_all),'five fold: ', acc_all)\n",
    "print(\"ROC AUC: \", np.mean(roc_acu_all), 'five fold: ', roc_acu_all)\n",
    "print(\"PR AUC: \", np.mean(pr_auc_all), 'five fold: ', pr_auc_all )\n",
    "print('Recall / Sensitivity: ', np.mean(recall_all), 'five fold: ', recall_all)\n",
    "print('Specificity: ', np.mean(spec_all), 'five fold: ', spec_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Test:  0.49909909909909905 five fold:  [0.5045045045045045, 0.5225225225225225, 0.5405405405405406, 0.45045045045045046, 0.4774774774774775]\n",
      "ROC AUC:  0.5 five fold:  [0.5, 0.5, 0.5, 0.5, 0.5]\n",
      "PR AUC:  0.5 five fold:  [0.5, 0.5, 0.5, 0.5, 0.5]\n",
      "Recall / Sensitivity:  0.0 five fold:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Specificity:  1.0 five fold:  [1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:993: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:993: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:993: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:993: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:993: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# CD4 TEST\n",
    "acc_all = []\n",
    "pr_auc_all = []\n",
    "roc_acu_all = []\n",
    "recall_all = []\n",
    "spec_all = []\n",
    "\n",
    "for i in range(5):\n",
    "    pred_i = 'cd4_pred_%s'%(i+1)\n",
    "    true_i = 'cd4_true_%s'%(i+1)\n",
    "    cd4_pred_i = np.array(cd4_results[pred_i])\n",
    "    cd4_true_i = np.array(cd4_results[true_i])\n",
    "    \n",
    "    acc_pred1, roc_auc, pr_auc, recall_scores, spec_score = cal_results(cd4_true_i, cd4_pred_i)\n",
    "\n",
    "    acc_all.append(acc_pred1)\n",
    "    pr_auc_all.append(pr_auc)\n",
    "    roc_acu_all.append(roc_auc)\n",
    "    recall_all.append(recall_scores)\n",
    "    spec_all.append(spec_score)\n",
    "\n",
    "print(\"Accuracy Test: \", np.mean(acc_all),'five fold: ', acc_all)\n",
    "print(\"ROC AUC: \", np.mean(roc_acu_all), 'five fold: ', roc_acu_all)\n",
    "print(\"PR AUC: \", np.mean(pr_auc_all), 'five fold: ', pr_auc_all )\n",
    "print('Recall / Sensitivity: ', np.mean(recall_all), 'five fold: ', recall_all)\n",
    "print('Specificity: ', np.mean(spec_all), 'five fold: ', spec_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Test:  0.4936936936936937 five fold:  [0.4954954954954955, 0.4954954954954955, 0.5675675675675675, 0.42342342342342343, 0.4864864864864865]\n",
      "ROC AUC:  0.5 five fold:  [0.5, 0.5, 0.5, 0.5, 0.5]\n",
      "PR AUC:  1.0 five fold:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Recall / Sensitivity:  1.0 five fold:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Specificity:  0.0 five fold:  [0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# ENDO TEST\n",
    "acc_all = []\n",
    "pr_auc_all = []\n",
    "roc_acu_all = []\n",
    "recall_all = []\n",
    "spec_all = []\n",
    "\n",
    "for i in range(5):\n",
    "    pred_i = 'endo_pred_%s'%(i+1)\n",
    "    true_i = 'endo_true_%s'%(i+1)\n",
    "    endo_pred_i = np.array(endo_results[pred_i])\n",
    "    endo_true_i = np.array(endo_results[true_i])\n",
    "    \n",
    "    acc_pred1, roc_auc, pr_auc, recall_scores, spec_score = cal_results(endo_true_i, endo_pred_i)\n",
    "\n",
    "    acc_all.append(acc_pred1)\n",
    "    pr_auc_all.append(pr_auc)\n",
    "    roc_acu_all.append(roc_auc)\n",
    "    recall_all.append(recall_scores)\n",
    "    spec_all.append(spec_score)\n",
    "\n",
    "print(\"Accuracy Test: \", np.mean(acc_all),'five fold: ', acc_all)\n",
    "print(\"ROC AUC: \", np.mean(roc_acu_all), 'five fold: ', roc_acu_all)\n",
    "print(\"PR AUC: \", np.mean(pr_auc_all), 'five fold: ', pr_auc_all )\n",
    "print('Recall / Sensitivity: ', np.mean(recall_all), 'five fold: ', recall_all)\n",
    "print('Specificity: ', np.mean(spec_all), 'five fold: ', spec_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Test:  0.5153153153153153 five fold:  [0.5045045045045045, 0.5405405405405406, 0.5045045045045045, 0.5135135135135135, 0.5135135135135135]\n",
      "ROC AUC:  0.5 five fold:  [0.5, 0.5, 0.5, 0.5, 0.5]\n",
      "PR AUC:  0.5 five fold:  [0.5, 0.5, 0.5, 0.5, 0.5]\n",
      "Recall / Sensitivity:  0.0 five fold:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Specificity:  1.0 five fold:  [1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:993: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:993: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:993: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:993: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:993: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# MACRO TEST\n",
    "acc_all = []\n",
    "pr_auc_all = []\n",
    "roc_acu_all = []\n",
    "recall_all = []\n",
    "spec_all = []\n",
    "\n",
    "for i in range(5):\n",
    "    pred_i = 'macro_pred_%s'%(i+1)\n",
    "    true_i = 'macro_true_%s'%(i+1)\n",
    "    macro_pred_i = np.array(macro_results[pred_i])\n",
    "    macro_true_i = np.array(macro_results[true_i])\n",
    "    \n",
    "    acc_pred1, roc_auc, pr_auc, recall_scores, spec_score = cal_results(macro_true_i, macro_pred_i)\n",
    "\n",
    "    acc_all.append(acc_pred1)\n",
    "    pr_auc_all.append(pr_auc)\n",
    "    roc_acu_all.append(roc_auc)\n",
    "    recall_all.append(recall_scores)\n",
    "    spec_all.append(spec_score)\n",
    "\n",
    "print(\"Accuracy Test: \", np.mean(acc_all),'five fold: ', acc_all)\n",
    "print(\"ROC AUC: \", np.mean(roc_acu_all), 'five fold: ', roc_acu_all)\n",
    "print(\"PR AUC: \", np.mean(pr_auc_all), 'five fold: ', pr_auc_all )\n",
    "print('Recall / Sensitivity: ', np.mean(recall_all), 'five fold: ', recall_all)\n",
    "print('Specificity: ', np.mean(spec_all), 'five fold: ', spec_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
